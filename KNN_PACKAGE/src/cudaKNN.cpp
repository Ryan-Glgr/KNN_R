#ifdef USE_CUDA
#include <iostream>
#include <fstream>
#include <vector>
#include <set>
#include <chrono>
#include <thread>
#include <cuda.h>
#include "library.h"
#include <Rcpp.h>
#pragma message("COMPILING CUDA!\n")

// Global settings and profiling variables
#define threadsPerBlock 128
#define threadCount 1

auto profileClock = std::chrono::high_resolution_clock::now();
long double runTime = 0;

// Helper function for profiling
long double calculateTime () {
    auto end = std::chrono::duration_cast<std::chrono::microseconds>(std::chrono::high_resolution_clock::now() - profileClock);
    profileClock = std::chrono::high_resolution_clock::now();
    long double val = (long double)end.count()/1000000.0;
    runTime += val;
    return val;
}

// Remove duplicates helper (same as before)
template<typename T>
size_t removeDuplicates(std::vector<T>& vec) {
    std::set<T> seen;
    auto newEnd = std::remove_if(vec.begin(), vec.end(), [&seen](const T& value) {
        if (seen.find(value) != std::end(seen))
            return true;
        seen.insert(value);
        return false;
    });
    vec.erase(newEnd, vec.end());
    return vec.size();
}

// --- CUDA Driver API Setup ---
// Load a PTX module and return the module handle
CUmodule loadModule(const char* ptxFile) {
    CUmodule module;
    CUresult res = cuModuleLoad(&module, ptxFile);
    if (res != CUDA_SUCCESS) {
        std::cerr << "Error loading PTX module: " << res << std::endl;
        exit(1);
    }
    return module;
}
// Retrieve a function from the loaded module
CUfunction getKernelFunction(CUmodule module, const char* kernelName) {
    CUfunction func;
    CUresult res = cuModuleGetFunction(&func, module, kernelName);
    if (res != CUDA_SUCCESS) {
        std::cerr << "Error getting kernel function " << kernelName << ": " << res << std::endl;
        exit(1);
    }
    return func;
}

// Launch a kernel using the driver API
void launchKernel(CUfunction kernel, void** args, int gridSize, int blockSize) {
    CUresult res = cuLaunchKernel(kernel,
                                  gridSize, 1, 1,   // grid dimensions
                                  blockSize, 1, 1,  // block dimensions
                                  0, 0,            // shared mem and stream
                                  args, 0);
    if (res != CUDA_SUCCESS) {
        std::cerr << "Kernel launch failed: " << res << std::endl;
        exit(1);
    }
    // Wait for kernel to finish
    cuCtxSynchronize();
}

// --- Host-side functions ---
// (Rewritten versions of your original functions; note that each kernel launch now
// uses launchKernel() with appropriate parameters from the loaded module.)

// Global variable to store a device pointer for results (if needed)
CUdeviceptr devResult;

// Example of a host function that mimics the kNN kernel launch behavior
void kNN(const std::vector<double>& x, int k, int writeLocation, double weight,
         CUmodule module, CUdeviceptr devMatrixX, int* prevMatrixXSize) {
    int N = x.size();
    if (k > N - 1)
        k = N - 1;
    if (N <= 1)
        return;

    // Reallocate devMatrixX if needed (omitted error checking for brevity)
    if (*prevMatrixXSize < N * N) {
        cuMemFree(devMatrixX);
        cuMemAlloc(&devMatrixX, sizeof(double) * N * N);
        *prevMatrixXSize = N * N;
    }

    // Allocate device memory for vector x
    CUdeviceptr devX;
    cuMemAlloc(&devX, sizeof(double) * N);
    cuMemcpyHtoD(devX, x.data(), sizeof(double) * N);

    // Launch multiWrite kernel from the PTX module
    CUfunction multiWriteKernel = getKernelFunction(module, "multiWrite");
    int gridSize = (N + threadsPerBlock - 1) / threadsPerBlock;
    void* args1[] = { &devMatrixX, &devX, &N };
    launchKernel(multiWriteKernel, args1, gridSize, threadsPerBlock);
    std::cout << "\tMultiWrite kernel launched at: " << calculateTime() << " seconds" << std::endl;

    // Launch distanceVectorize kernel
    CUfunction distanceKernel = getKernelFunction(module, "distanceVectorize");
    void* args2[] = { &devMatrixX, &N };
    launchKernel(distanceKernel, args2, gridSize, threadsPerBlock);
    std::cout << "\tDistanceVectorize kernel launched at: " << calculateTime() << " seconds" << std::endl;

    // Launch gpukNN kernel
    CUfunction gpukNNKernel = getKernelFunction(module, "gpukNN");
    void* args3[] = { &devMatrixX, &N, &k, &devResult, &weight };
    launchKernel(gpukNNKernel, args3, gridSize, threadsPerBlock);
    std::cout << "\tgpukNN kernel launched at: " << calculateTime() << " seconds" << std::endl;

    // Cleanup temporary allocation
    cuMemFree(devX);
}

// (Similarly, rewrite threadRun() and run() to use these helper routines.)
// For brevity, hereâ€™s an abbreviated version of run():

void run(double* data_x, int size1, double* data_y, int size2, int k) {
    // Build unique values vector from data_y
    std::vector<double> yval(data_y, data_y + size2);
    removeDuplicates(yval);
    calculateTime();

    // Set a high heap size if needed (using cudaDeviceSetLimit with runtime API would not be used here)
    // For driver API you might need to adjust context properties accordingly.

    // Allocate device memory for devResult (one double per unique y value)
    cuMemAlloc(&devResult, sizeof(double) * yval.size());
    cuMemsetD32(devResult, 0, yval.size());

    // Create module from the compiled PTX file. this way we can load in our .ptx dynamically at runtime. this allows cross platform much easier.
    // have to use a built in Rcpp function because R moves things around in your directories when you are installing packages. 
    Rcpp::Function sysFile("system.file");
    std::string kernelsPath = Rcpp::as<std::string>(
        sysFile("kernels", "cudaKernels.ptx", Rcpp::Named("package") = "CWUKNN")
    );
    std::ifstream kernelFile(kernelsPath);
    // Error value if there is failure opening the kernel
    if (!kernelFile.is_open()) {
        Rcpp::Rcerr << "Failed to open kernel file cudaKernels.ptx!" << std::endl;
        return;
    }
    std::string kernelFileName = kernelsPath;
    CUmodule module = loadModule(kernelFileName.c_str());


    // Allocate a matrix for kNN calculations
    int prevMatrixXSize = yval.size() * yval.size();
    CUdeviceptr devMatrixX;
    cuMemAlloc(&devMatrixX, sizeof(double) * prevMatrixXSize);

    // Launch threads (here we use std::thread for concurrency)
    std::thread threads[threadCount];
    for (int t = 0; t < threadCount; t++) {
        threads[t] = std::thread([=, &prevMatrixXSize, module]() {
            // Each thread processes a subset of yval.
            // (Insert your threadRun() code here that builds equality vectors and calls kNN.)
            // For brevity, a simplified loop:
            for (int i = t; i < yval.size(); i += threadCount) {
                std::vector<double> x;
                for (int a = 0; a < size1; a++) {
                    if (data_y[a] == yval[i])
                        x.push_back(data_x[a]);
                }
                kNN(x, k, i, (double)x.size() / (double)size1, module, devMatrixX, &prevMatrixXSize);
            }
        });
    }
    for (int t = 0; t < threadCount; t++) {
        threads[t].join();
    }

    // Retrieve and sum the results
    std::vector<double> resultArr(yval.size());
    cuMemcpyDtoH(resultArr.data(), devResult, sizeof(double) * yval.size());
    cuMemFree(devResult);
    cuMemFree(devMatrixX);
    cuModuleUnload(module);

    double finalResult = 0;
    for (auto r : resultArr)
        finalResult += r;

    std::cout << "\nFinal Result: " << finalResult << std::endl;
}

// This is the externally visible function that wraps run().
// It is declared extern "C" so it can be linked elsewhere.
double cudaKNN(double* vec1, int size1, double* vec2, int size2, int K){
    // Initialize the CUDA Driver API and create a context.
    cuInit(0);
    CUcontext context;
    cuCtxCreate(&context, 0, 0);

    // Call our run function
    run(vec1, size1, vec2, size2, K);

    // Clean up the context (if needed)
    cuCtxDestroy(context);
    return 0; // or return an appropriate result
}

#endif